package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/tmc/langchaingo/llms/openai"

	"reactsql/internal/adapter"
	"reactsql/internal/agent"
	contextpkg "reactsql/internal/context"
	"reactsql/internal/logger"
)

func main() {
	// è§£æå‘½ä»¤è¡Œå‚æ•°
	dbDir := flag.String("db-dir", "benchmarks/bird/dev/dev_databases", "BIRDæ•°æ®åº“ç›®å½•")
	outputDir := flag.String("output-dir", "contexts/sqlite/bird", "è¾“å‡ºç›®å½•")
	dbName := flag.String("db", "", "æŒ‡å®šå•ä¸ªæ•°æ®åº“åç§°ï¼ˆä¸ºç©ºåˆ™å¤„ç†å…¨éƒ¨ï¼‰")
	workers := flag.Int("workers", 3, "å¹¶å‘å¤„ç†çš„æ•°æ®åº“æ•°é‡")
	skipExisting := flag.Bool("skip-existing", true, "è·³è¿‡å·²å­˜åœ¨çš„Rich Contextæ–‡ä»¶")
	useV32 := flag.Bool("v3.2", false, "ä½¿ç”¨ DeepSeek-V3.2 æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨ V3ï¼‰")

	flag.Parse()

	// æ ¹æ®æ ‡å¿—é€‰æ‹©æ¨¡å‹
	modelName := "deepseek-v3-250324"
	modelDisplay := "DeepSeek-V3"
	if *useV32 {
		modelName = "deepseek-v3-2-251201"
		modelDisplay = "DeepSeek-V3.2"
	}

	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	fmt.Println("ğŸš€ BIRD Rich Context Generator")
	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	fmt.Printf("æ•°æ®åº“ç›®å½•: %s\n", *dbDir)
	fmt.Printf("è¾“å‡ºç›®å½•: %s\n", *outputDir)
	fmt.Printf("å¹¶å‘æ•°: %d\n", *workers)
	fmt.Printf("ğŸ¤– Model: %s\n", modelDisplay)
	fmt.Println()

	// åˆ›å»ºè¾“å‡ºç›®å½•
	if err := os.MkdirAll(*outputDir, 0755); err != nil {
		log.Fatalf("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: %v", err)
	}

	// è·å–æ‰€æœ‰æ•°æ®åº“
	var databases []string
	if *dbName != "" {
		// å¤„ç†å•ä¸ªæ•°æ®åº“
		databases = []string{*dbName}
	} else {
		// è·å–æ‰€æœ‰æ•°æ®åº“
		entries, err := os.ReadDir(*dbDir)
		if err != nil {
			log.Fatalf("è¯»å–æ•°æ®åº“ç›®å½•å¤±è´¥: %v", err)
		}

		for _, entry := range entries {
			if entry.IsDir() {
				databases = append(databases, entry.Name())
			}
		}
	}

	fmt.Printf("æ‰¾åˆ° %d ä¸ªæ•°æ®åº“\n\n", len(databases))

	// è¿‡æ»¤å·²å­˜åœ¨çš„
	if *skipExisting {
		var toProcess []string
		for _, db := range databases {
			outputFile := filepath.Join(*outputDir, db+".json")
			if _, err := os.Stat(outputFile); os.IsNotExist(err) {
				toProcess = append(toProcess, db)
			} else {
				fmt.Printf("â­ï¸  è·³è¿‡ %s (å·²å­˜åœ¨)\n", db)
			}
		}
		databases = toProcess
		fmt.Printf("\néœ€è¦å¤„ç† %d ä¸ªæ•°æ®åº“\n\n", len(databases))
	}

	if len(databases) == 0 {
		fmt.Println("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ•°æ®åº“")
		return
	}

	// åˆ›å»ºLLM
	llm, err := openai.New(
		openai.WithModel(modelName),
		openai.WithToken("404b0d95-e938-4fbb-8724-34d2f0dadb00"),
		openai.WithBaseURL("https://ark.cn-beijing.volces.com/api/v3"),
	)
	if err != nil {
		log.Fatalf("åˆå§‹åŒ–LLMå¤±è´¥: %v", err)
	}

	// å¹¶å‘å¤„ç†
	startTime := time.Now()
	var wg sync.WaitGroup
	semaphore := make(chan struct{}, *workers)
	successCount := 0
	failCount := 0
	var mu sync.Mutex

	for i, dbName := range databases {
		wg.Add(1)
		semaphore <- struct{}{} // è·å–ä¿¡å·é‡

		go func(idx int, name string) {
			defer wg.Done()
			defer func() { <-semaphore }() // é‡Šæ”¾ä¿¡å·é‡

			fmt.Printf("[%d/%d] ğŸ”„ å¤„ç† %s ...\n", idx+1, len(databases), name)

			if err := processDatabase(llm, *dbDir, *outputDir, name); err != nil {
				fmt.Printf("[%d/%d] âŒ å¤±è´¥ %s: %v\n", idx+1, len(databases), name, err)
				mu.Lock()
				failCount++
				mu.Unlock()
			} else {
				fmt.Printf("[%d/%d] âœ… å®Œæˆ %s\n", idx+1, len(databases), name)
				mu.Lock()
				successCount++
				mu.Unlock()
			}
		}(i, dbName)
	}

	wg.Wait()
	duration := time.Since(startTime)

	// æ‰“å°æ‘˜è¦
	fmt.Println("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	fmt.Println("ğŸ“Š å¤„ç†å®Œæˆ")
	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	fmt.Printf("æ€»æ•°: %d\n", len(databases))
	fmt.Printf("æˆåŠŸ: %d\n", successCount)
	fmt.Printf("å¤±è´¥: %d\n", failCount)
	fmt.Printf("è€—æ—¶: %v\n", duration)
	fmt.Printf("å¹³å‡: %.2fs/æ•°æ®åº“\n", duration.Seconds()/float64(len(databases)))
	fmt.Println()
	fmt.Printf("âœ… Rich Context æ–‡ä»¶ä¿å­˜åœ¨: %s\n", *outputDir)
}

func processDatabase(llm *openai.LLM, dbDir, outputDir, dbName string) error {
	ctx := context.Background()

	// 1. åˆ›å»ºæ•°æ®åº“é€‚é…å™¨
	dbPath := filepath.Join(dbDir, dbName, dbName+".sqlite")
	dbAdapter, err := adapter.NewAdapter(&adapter.DBConfig{
		Type:     "sqlite",
		FilePath: dbPath,
	})
	if err != nil {
		return fmt.Errorf("åˆ›å»ºadapterå¤±è´¥: %w", err)
	}

	if err := dbAdapter.Connect(ctx); err != nil {
		return fmt.Errorf("è¿æ¥æ•°æ®åº“å¤±è´¥: %w", err)
	}
	defer dbAdapter.Close()

	// 2. åˆ›å»ºSharedContext
	sharedCtx := contextpkg.NewSharedContext(dbName, "sqlite")

	// 3. Phase 1: Coordinator Agentå‘ç°è¡¨
	progLogger := logger.NewLogger(0)
	progLogger.SetPhase(fmt.Sprintf("[%s] Phase 1: Discovering Tables", dbName))

	coordinator, err := agent.NewCoordinatorAgent("coordinator", llm, dbAdapter, sharedCtx)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºcoordinatorå¤±è´¥: %w", err)
	}

	if err := coordinator.Execute(ctx); err != nil {
		return fmt.Errorf("coordinatoræ‰§è¡Œå¤±è´¥: %w", err)
	}

	// 4. Phase 2: Worker Agentsåˆ†æè¡¨
	tasks := sharedCtx.GetAllTasks()
	var workerTasks []*contextpkg.TaskInfo
	for _, task := range tasks {
		if task.AgentID != "coordinator" {
			workerTasks = append(workerTasks, task)
		}
	}

	progLogger = logger.NewLogger(len(workerTasks))
	progLogger.SetPhase(fmt.Sprintf("[%s] Phase 2: Analyzing %d Tables", dbName, len(workerTasks)))

	var wg sync.WaitGroup
	for _, task := range workerTasks {
		tableName := task.ID[8:] // å»æ‰ "analyze_" å‰ç¼€

		wg.Add(1)
		go func(taskID, agentID, tblName string) {
			defer wg.Done()

			progLogger.StartTask(tblName)

			worker, err := agent.NewWorkerAgent(agentID, taskID, tblName, llm, dbAdapter, sharedCtx)
			if err != nil {
				progLogger.FailTask(tblName, err)
				return
			}

			if err := worker.Execute(ctx); err != nil {
				progLogger.FailTask(tblName, err)
				return
			}

			progLogger.CompleteTask(tblName)
		}(task.ID, task.AgentID, tableName)
	}

	wg.Wait()

	// 5. ä¿å­˜ç»“æœ
	outputFile := filepath.Join(outputDir, dbName+".json")
	if err := sharedCtx.SaveToFile(outputFile); err != nil {
		return fmt.Errorf("ä¿å­˜æ–‡ä»¶å¤±è´¥: %w", err)
	}

	return nil
}
