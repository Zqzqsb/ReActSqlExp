package inference

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"time"

	"github.com/pkoukk/tiktoken-go"
	"github.com/tmc/langchaingo/llms"

	"reactsql/internal/adapter"
	contextpkg "reactsql/internal/context"
)

// Config æ¨ç†ç®¡çº¿é…ç½®
type Config struct {
	UseRichContext bool
	UseReact       bool
	ReactLinking   bool // Schema Linking æ˜¯å¦ä½¿ç”¨ ReAct æ¨¡å¼
	UseDryRun      bool
	MaxIterations  int
	ContextFile    string

	// æ¾„æ¸…åŠŸèƒ½é…ç½®
	ClarifyMode             string   // æ¾„æ¸…æ¨¡å¼: "off" (ä¸å¯ç”¨) | "on" (agentä¸»åŠ¨è¯¢é—®) | "force" (å¼ºåˆ¶ç»™å‡º)
	LogMode                 string   // æ—¥å¿—æ¨¡å¼: "simple" (ç®€æ´) | "full" (å®Œæ•´è¾“å‡ºæ‰€æœ‰äº¤äº’)
	ResultFields            []string // æœŸæœ›çš„ç»“æœå­—æ®µåˆ—è¡¨
	ResultFieldsDescription string   // ç»“æœå­—æ®µçš„æè¿°

	// æ ¡å¯¹æ¨¡å¼é…ç½®
	EnableProofread bool   // æ˜¯å¦å¯ç”¨æ ¡å¯¹æ¨¡å¼ï¼ˆå…è®¸ LLM ä¿®æ­£ Rich Contextï¼‰
	DBName          string // æ•°æ®åº“åç§°
	DBType          string // æ•°æ®åº“ç±»å‹
}

// StepCallback is called for each ReAct step update during streaming
// eventType: "thought" | "action" | "observation" | "finish"
type StepCallback func(step ReActStep, eventType string)

// Pipeline æ¨ç†ç®¡çº¿
type Pipeline struct {
	llm          llms.Model
	adapter      adapter.DBAdapter
	config       *Config
	context      *contextpkg.SharedContext
	schemaLinker SchemaLinker
	tokenizer    *tiktoken.Tiktoken

	// Token ç»Ÿè®¡ç´¯ç§¯å™¨
	promptTexts   []string
	responseTexts []string

	// Streaming callback
	stepCallback StepCallback
}

// Result æ¨ç†ç»“æœ
type Result struct {
	Query           string
	GeneratedSQL    string
	ExecutionResult interface{}

	// ç»Ÿè®¡ä¿¡æ¯
	TotalTime     time.Duration
	LLMCalls      int
	SQLExecutions int
	TotalTokens   int
	ClarifyCount  int // æ¾„æ¸…æ¬¡æ•°

	// ä¸­é—´ç»“æœ
	SelectedTables []string
	ReActSteps     []ReActStep
}

// ReActStep ReAct æ­¥éª¤
type ReActStep struct {
	Step        int         `json:"step,omitempty"`              // Step number for streaming
	Thought     string      `json:"thought"`
	Action      string      `json:"action"`
	ActionInput interface{} `json:"action_input,omitempty"` // æ”¯æŒ string å’Œ map[string]interface{}
	Observation string      `json:"observation,omitempty"`
	Phase       string      `json:"phase,omitempty"` // "schema_linking" or "sql_generation"
}

// Reset æ¸…ç†ç´¯ç§¯çš„ç»Ÿè®¡æ•°æ®ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
func (p *Pipeline) Reset() {
	p.promptTexts = nil
	p.responseTexts = nil
	p.stepCallback = nil
}

// SetStepCallback sets the callback function for streaming ReAct steps
func (p *Pipeline) SetStepCallback(callback StepCallback) {
	p.stepCallback = callback
}

// notifyStep notifies the callback of a ReAct step update
func (p *Pipeline) notifyStep(step ReActStep, eventType string) {
	if p.stepCallback != nil {
		p.stepCallback(step, eventType)
	}
}

// NewPipeline åˆ›å»ºæ¨ç†ç®¡çº¿
func NewPipeline(llm llms.Model, adapter adapter.DBAdapter, config *Config) *Pipeline {
	// åˆå§‹åŒ– tokenizer (ä½¿ç”¨ cl100k_baseï¼Œé€‚ç”¨äº GPT-3.5/GPT-4/DeepSeek)
	tokenizer, err := tiktoken.GetEncoding("cl100k_base")
	if err != nil {
		// å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ nilï¼Œåç»­ä¼šè·³è¿‡ token ç»Ÿè®¡
		tokenizer = nil
	}

	// Schema Linking ä½¿ç”¨ ReAct æ¨¡å¼ï¼ˆç”± ReactLinking é…ç½®æ§åˆ¶ï¼‰
	linker := NewLLMSchemaLinker(llm, adapter, config.ReactLinking)

	p := &Pipeline{
		llm:          llm,
		adapter:      adapter,
		config:       config,
		schemaLinker: linker,
		tokenizer:    tokenizer,
	}

	// è®¾ç½® token recorder
	linker.tokenRecorder = func(prompt, response string) {
		p.promptTexts = append(p.promptTexts, prompt)
		p.responseTexts = append(p.responseTexts, response)
	}

	// åŠ è½½ Context æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
	// æ³¨æ„ï¼šcontext æ€»æ˜¯åŠ è½½ç”¨äº Schema Linking
	// UseRichContext åªæ§åˆ¶æ˜¯å¦åœ¨ SQL Generation ä¸­ä½¿ç”¨ rich_context
	if config.ContextFile != "" {
		if ctx, err := p.loadContext(config.ContextFile); err == nil {
			p.context = ctx
		}
	}

	return p
}

// countTokens ç»Ÿè®¡æ–‡æœ¬çš„ token æ•°é‡
func (p *Pipeline) countTokens(text string) int {
	if p.tokenizer == nil {
		return 0
	}
	tokens := p.tokenizer.Encode(text, nil, nil)
	return len(tokens)
}

// Execute æ‰§è¡Œæ¨ç†
func (p *Pipeline) Execute(ctx context.Context, query string) (*Result, error) {
	startTime := time.Now()

	// é‡ç½® token ç»Ÿè®¡ç´¯ç§¯å™¨
	p.promptTexts = []string{}
	p.responseTexts = []string{}

	result := &Result{
		Query:      query,
		ReActSteps: []ReActStep{},
	}

	// 1. Schema Linking (æ€»æ˜¯æ‰§è¡Œï¼Œè¯†åˆ«ç›¸å…³è¡¨)
	var allTableInfo map[string]*TableInfo
	var err error
	if p.context != nil {
		// ä» Rich Context æå–è¡¨ä¿¡æ¯
		allTableInfo = ExtractTableInfo(p.context)
	} else {
		// ä»æ•°æ®åº“æŸ¥è¯¢è¡¨ä¿¡æ¯
		allTableInfo, err = p.extractTableInfoFromDB(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to extract table info: %w", err)
		}
	}

	tables, schemaLinkingSteps, err := p.schemaLinker.Link(ctx, query, allTableInfo)
	if err != nil {
		return nil, fmt.Errorf("schema linking failed: %w", err)
	}
	result.SelectedTables = tables
	result.LLMCalls++

	// Add Schema Linking ReAct steps to result
	for _, step := range schemaLinkingSteps {
		result.ReActSteps = append(result.ReActSteps, ReActStep{
			Thought:     step.Thought,
			Action:      step.Action,
			ActionInput: step.ActionInput,
			Observation: step.Observation,
			Phase:       "schema_linking",
		})
	}

	fmt.Printf("ğŸ“‹ Selected Tables: %v\n\n", tables)

	// 2. æ„å»º Schema Context (åŸºç¡€è¡¨ç»“æ„ä¿¡æ¯ï¼Œæ€»æ˜¯æä¾›)
	var contextPrompt string

	if p.config.UseRichContext && p.context != nil {
		// ä½¿ç”¨ Rich Context (è¯¦ç»†ä¿¡æ¯)
		opts := &contextpkg.ExportOptions{
			Tables:             tables,
			IncludeColumns:     true,
			IncludeIndexes:     true,
			IncludeRichContext: true,
			IncludeStats:       true,
		}
		contextPrompt = p.context.ExportToCompactPrompt(opts)
		// ä¸æ‰“å°å®Œæ•´çš„ Rich Contextï¼Œåªæ‰“å°ç®€è¦ä¿¡æ¯
		fmt.Printf("ğŸ“š Using Rich Context for %d tables\n", len(tables))
	} else {
		// ä½¿ç”¨åŸºç¡€ Schema (ä»…è¡¨å+åˆ—å)
		contextPrompt = p.buildBasicSchema(ctx, tables)
		// ä¸æ‰“å°å®Œæ•´çš„ Basic Schema
		fmt.Printf("ğŸ“‹ Using Basic Schema for %d tables\n", len(tables))
	}

	// 3. Generate SQL
	var sql string
	if p.config.UseReact {
		sql, err = p.reactLoop(ctx, query, contextPrompt, result)
	} else {
		sql, err = p.oneShotGeneration(ctx, query, contextPrompt)
		result.LLMCalls++
	}

	if err != nil {
		return nil, fmt.Errorf("SQL generation failed: %w", err)
	}

	result.GeneratedSQL = sql
	result.TotalTime = time.Since(startTime)

	// 4. ç»Ÿè®¡ tokensï¼ˆä»ç´¯ç§¯å™¨ä¸­ç»Ÿè®¡æ‰€æœ‰ prompts å’Œ responsesï¼‰
	// æš‚æ—¶ç¦ç”¨ token ç»Ÿè®¡ï¼Œé¿å…æ½œåœ¨çš„é—®é¢˜
	fmt.Printf("[DEBUG] Token counting disabled (would count %d prompts, %d responses)\n", len(p.promptTexts), len(p.responseTexts))
	result.TotalTokens = 0 // æš‚æ—¶è®¾ä¸º 0

	// if p.tokenizer != nil {
	// 	for i, prompt := range p.promptTexts {
	// 		fmt.Printf("[DEBUG] Counting prompt %d/%d (length: %d)\n", i+1, len(p.promptTexts), len(prompt))
	// 		result.TotalTokens += p.countTokens(prompt)
	// 	}
	// 	for i, response := range p.responseTexts {
	// 		fmt.Printf("[DEBUG] Counting response %d/%d (length: %d)\n", i+1, len(p.responseTexts), len(response))
	// 		result.TotalTokens += p.countTokens(response)
	// 	}
	// }

	// 5. Execute SQL (optional)
	if sql != "" {
		execResult, err := p.adapter.ExecuteQuery(ctx, sql)
		if err == nil {
			result.ExecutionResult = execResult
			result.SQLExecutions++
		}
	}

	return result, nil
}

// loadContext åŠ è½½ Rich Context
func (p *Pipeline) loadContext(path string) (*contextpkg.SharedContext, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return nil, err
	}

	var ctx contextpkg.SharedContext
	if err := json.Unmarshal(data, &ctx); err != nil {
		return nil, err
	}

	return &ctx, nil
}

// extractTableInfoFromDB ä»æ•°æ®åº“æå–è¡¨ä¿¡æ¯
func (p *Pipeline) extractTableInfoFromDB(ctx context.Context) (map[string]*TableInfo, error) {
	// è·å–æ‰€æœ‰è¡¨å
	var query string
	switch p.adapter.GetDatabaseType() {
	case "MySQL":
		query = "SHOW TABLES"
	case "PostgreSQL":
		query = "SELECT tablename FROM pg_tables WHERE schemaname='public'"
	case "SQLite":
		query = "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
	default:
		return nil, fmt.Errorf("unsupported database type")
	}

	result, err := p.adapter.ExecuteQuery(ctx, query)
	if err != nil {
		return nil, err
	}

	tableInfo := make(map[string]*TableInfo)

	// å¯¹æ¯ä¸ªè¡¨æŸ¥è¯¢åˆ—ä¿¡æ¯
	for _, row := range result.Rows {
		var tableName string
		for _, val := range row {
			if name, ok := val.(string); ok {
				tableName = name
				break
			}
		}

		if tableName == "" {
			continue
		}

		// æŸ¥è¯¢åˆ—ä¿¡æ¯
		var colQuery string
		switch p.adapter.GetDatabaseType() {
		case "MySQL":
			colQuery = fmt.Sprintf("DESCRIBE %s", tableName)
		case "SQLite":
			colQuery = fmt.Sprintf("PRAGMA table_info(%s)", tableName)
		case "PostgreSQL":
			colQuery = fmt.Sprintf("SELECT column_name FROM information_schema.columns WHERE table_name='%s'", tableName)
		}

		colResult, err := p.adapter.ExecuteQuery(ctx, colQuery)
		if err != nil {
			continue
		}

		columns := make([]string, 0, len(colResult.Rows))
		for _, colRow := range colResult.Rows {
			var colName string
			switch p.adapter.GetDatabaseType() {
			case "MySQL":
				if field, ok := colRow["Field"].(string); ok {
					colName = field
				}
			case "SQLite":
				if name, ok := colRow["name"].(string); ok {
					colName = name
				}
			case "PostgreSQL":
				if name, ok := colRow["column_name"].(string); ok {
					colName = name
				}
			}

			if colName != "" {
				columns = append(columns, colName)
			}
		}

		tableInfo[tableName] = &TableInfo{
			Name:    tableName,
			Columns: columns,
		}
	}

	return tableInfo, nil
}

// buildBasicSchema æ„å»ºåŸºç¡€ Schemaï¼ˆä»æ•°æ®åº“æŸ¥è¯¢è¡¨ç»“æ„ï¼‰
func (p *Pipeline) buildBasicSchema(ctx context.Context, tables []string) string {
	var sb strings.Builder

	sb.WriteString(fmt.Sprintf("Database: %s\n\n", p.adapter.GetDatabaseType()))

	for _, tableName := range tables {
		// æŸ¥è¯¢è¡¨ç»“æ„
		var query string
		switch p.adapter.GetDatabaseType() {
		case "MySQL":
			query = fmt.Sprintf("DESCRIBE %s", tableName)
		case "SQLite":
			query = fmt.Sprintf("PRAGMA table_info(%s)", tableName)
		case "PostgreSQL":
			query = fmt.Sprintf("SELECT column_name, data_type FROM information_schema.columns WHERE table_name='%s'", tableName)
		default:
			continue
		}

		result, err := p.adapter.ExecuteQuery(ctx, query)
		if err != nil {
			continue
		}

		// æ ¼å¼åŒ–è¡¨ç»“æ„
		sb.WriteString(fmt.Sprintf("Table %s:\n", tableName))

		for _, row := range result.Rows {
			var colName, colType string

			// æ ¹æ®æ•°æ®åº“ç±»å‹æå–åˆ—åå’Œç±»å‹
			switch p.adapter.GetDatabaseType() {
			case "MySQL":
				if field, ok := row["Field"].(string); ok {
					colName = field
				}
				if typ, ok := row["Type"].(string); ok {
					colType = typ
				}
			case "SQLite":
				if name, ok := row["name"].(string); ok {
					colName = name
				}
				if typ, ok := row["type"].(string); ok {
					colType = typ
				}
			case "PostgreSQL":
				if name, ok := row["column_name"].(string); ok {
					colName = name
				}
				if typ, ok := row["data_type"].(string); ok {
					colType = typ
				}
			}

			if colName != "" {
				sb.WriteString(fmt.Sprintf("  - %s: %s\n", colName, colType))
			}
		}

		sb.WriteString("\n")
	}

	return sb.String()
}
